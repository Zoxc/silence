use "lexer"
use "type-ast"

data Parser
	Lexer* lexer
	Compiler* compiler

	action create(lexer) :
		lexer <- lexer,
		compiler <- lexer.compiler

	fn tok()
		return lexer.tok.type

	fn eq(Token v)
		return v == tok()

	fn format_token() -> String
		return tok().str()

	fn step()
		lexer.next_token()

	fn array[T]() -> Array[T, Region]
		return Array[T, Region](lexer.region())

	fn ast[T, R] *args
		var R* r = Callable(lexer.region().new[T]).apply(args)
		return r

	fn extend(SrcLoc* src)
		match *src as s
			when SrcLoc.File
				s.stop = lexer.tok.last_ended
			else

	fn expected_str_s(SrcLoc* src, String str)
		lexer.report[Error.ExpectedString](src, str.copy(compiler.region.ref()))

	fn expected_str(String str)
		expected_str_s(lexer.src(), str)

	fn expected(Token tok)
		lexer.report[Error.ExpectedToken](lexer.src(), tok, format_token().copy(compiler.region.ref()))

	fn ensure(Token v)
		if v == tok()
			step()
		else
			expected(v)

	fn skip_line()
		if tok() == Token.Line
			lexer.next_token_after_line()

	fn get_ident()
		return if eq(Token.Ident)
			var r = lexer.tok.symbol
			step()
			r
		else
			expected(Token.Ident)
			Symbol.none

	fn matches(token)
		return if tok() == token
			step()
			true
		else
			false

	fn matches_ident(Symbol *ident)
		return if tok() == Token.Ident
			if ident == lexer.tok.symbol
				step()
				true
			else
				false
		else
			false

	fn sep()
		if eq(Token.Line)
			lexer.next_token_after_line()
		else
			expected(Token.Line)

	fn src(a)
		var s = lexer.src()
		var r = a(s)
		extend(s)
		return r

	# Global scope and program

	fn program()
		skip_line()
		global_entries -> tok() == Token.EndOfFile
		if tok() != Token.EndOfFile
			expected(Token.EndOfFile)

	fn global_entries(term)
		loop
			if !global_entry()
					break
			if term()
				break
			sep()

	fn global_entry() -> bool
		match tok()
			when Token.Ident
				match lexer.tok.symbol
					when Symbol.kw_data
						data()
						return true
					else
						attribute()
						return true
			else
				return false

		return true

	fn properties()
		loop
			if matches_ident(Symbol.kw_shared)
			else if matches_ident(Symbol.kw_import)
			else if matches_ident(Symbol.kw_export)
			else
				if matches(Token.SquareOpen)
					skip_line()

					if !eq(Token.SquareClose)
						loop
							get_ident()

							if matches(Token.Comma)
								skip_line()
							else
								break

					ensure(Token.SquareClose)

				return

	fn attribute()
		src \s ->
			var props = properties()
			var baseline = lexer.tok.indent

			if(matches_ident(Symbol.kw_fn))
				fn(s, baseline, get_ident(), props, nil)
			else
				var name = undef()
				var type = undef()
				var val
				(name, type) = name_opt_type()

				if matches(Token.Assign)
					skip_line()
					val = some expression()

	fn fn_params()
		skip_line()

		if is_type_expression()
			loop
				name_opt_type()

				if matches(Token.Comma)
					skip_line()
				else
					break

	fn fn(s, baseline, name, props, Option[Symbol*] action_type)
		var tp = kind_params()

		if matches(Token.Mul)
			src \s ->
				name_opt_type()
		else
			if action_type.has() and !eq(Token.ParentOpen)
			else
				ensure(Token.ParentOpen)
				fn_params()
				ensure(Token.ParentClose)

		if action_type.has()
			if matches(Token.Colon)
				skip_line()

				loop
					get_ident()
					ensure(Token.ArrowLeft)
					expression()

					if matches(Token.Comma)
						skip_line()
					else
						break
		else
			if matches(Token.ArrowRight)
				skip_line()

				type_expression()

				matches_ident(Symbol.kw_constraints)

		group(baseline)

	fn data()
		var s = lexer.src()
		var baseline = lexer.tok.indent
		step()
		var sizeable = !matches_ident(Symbol.kw_ref)
		var name = get_ident()
		kind_params()
		extend s
		var scope = global_scope(baseline)

	fn kind_params() -> () constraints
		src \s ->
			if matches(Token.SquareOpen)
				skip_line()
				var params = type_params()
				ensure(Token.SquareClose)

	fn type_args(Array[TypeNode*, Region]* args)

		loop
			type_operator()
			if matches(Token.Comma)
				skip_line()
			else
				break

	fn type_params()
		src \s ->
			var type = type_expression()
			var value = false
			var name = Symbol.none
			if eq(Token.Ident)
				name = get_ident()
			else if matches(Token.DoubleColon)
				skip_line()
				name = get_ident()
				value = true
			else
				type = TypeNode.none

			kind_params()
			if matches(Token.Assign)
				skip_line()
				if value
					expression
				else
					type_expression

	fn is_type_expression()
		return match tok()
			when Token.Ident, Token.Number, Token.String, Token.ParentOpen
				true
			else
				false

	fn type_operator()
		var r = type_expression()

		loop
			if !eq(Token.ArrowRight)
				break

			step()
			skip_line()

			type_expression()

		return r

	fn type_expression() -> TypeNode* constraints
		type_ptr()

	fn type_ptr()
		var r = type_chain()

		loop
			if !eq(Token.Mul)
				break

			step()

		return r

	fn type_chain()
		var r = type_factor()

		loop
			match tok()
				when Token.SquareOpen
					step()
					skip_line()
					var args = array()
					type_args(&args)
					ensure(Token.SquareClose)
				when Token.Dot
					step()
					skip_line()
					var name = get_ident()
				else
					break

		return r

	fn type_tuple(s)
		step()
		skip_line()

		var nodes = array()

		if !eq(Token.ParentClose)
			type_args(&nodes)

		ensure(Token.ParentClose)

		return ast[TypeNode.Tuple](s, nodes)

	fn type_factor()
		return src \s ->
			return match tok()
				when Token.ParentOpen
					type_tuple(s)
				when Token.Number
					var r = ast[TypeNode.Number](s)
					step()
					r
				when Token.Ident
					if matches_ident(Symbol.kw_typeof)
						chain()
						ast[TypeNode.Typeof](s)
					else
						var r2 = ast[TypeNode.Ref](s, lexer.tok.symbol)
						step()
						r2
				else
					expected_str("type")
					TypeNode.none

	fn name_opt_type()
		var type = type_expression()
		var name = undef()

		if eq(Token.Ident)
			name = get_ident()
		else
			match *type as node
				when TypeNode.Ref
					name = node.name
					type = TypeNode.none
				else
					expected_str_s(type.src, "variable name")

		return (name, type)


	fn global_scope(baseline)

	fn group(baseline)

	fn expression()

	fn chain()
use "lexer"
use "type-ast"
use "value-ast"
use "def-ast"

data Parser
	Lexer* lexer
	Compiler* compiler

	action create(lexer) :
		lexer <- lexer,
		compiler <- lexer.compiler

		setup_ops()

	fn tok()
		return lexer.tok.type

	fn eq(Token v)
		return v == tok()

	fn eq_i(Symbol *ident)
		return eq(Token.Ident) and lexer.tok.symbol == ident

	fn format_token() -> String
		return tok().str()

	fn step()
		lexer.next_token()

	fn array[T]() -> Array[T, Region]
		return Array[T, Region](lexer.region())

	fn ast[T, R] *args
		var R* r = Callable(lexer.region().new[T]).apply(args)
		return r

	fn extend(SrcLoc* src)
		match *src as s
			when SrcLoc.File
				s.stop = lexer.tok.last_ended
			else

	fn expected_str_s(SrcLoc* src, String str)
		lexer.report[Error.ExpectedString](src, str.copy(compiler.region.ref()))

	fn expected_str(String str)
		expected_str_s(lexer.src(), str)

	fn expected(Token tok)
		lexer.report[Error.ExpectedToken](lexer.src(), tok, format_token().copy(compiler.region.ref()))

	fn ensure(Token v)
		if v == tok()
			step()
		else
			expected(v)

	fn skip_line()
		if tok() == Token.Line
			lexer.next_token_after_line()

	fn get_ident()
		return if eq(Token.Ident)
			var r = lexer.tok.symbol
			step()
			r
		else
			expected(Token.Ident)
			Symbol.none

	fn matches(token)
		return if tok() == token
			step()
			true
		else
			false

	fn matches_ident(Symbol *ident)
		return if tok() == Token.Ident
			if ident == lexer.tok.symbol
				step()
				true
			else
				false
		else
			false

	fn sep()
		if eq(Token.Line)
			lexer.next_token_after_line()
		else
			expected(Token.Line)

	fn src(a)
		var s = lexer.src()
		var r = a(s)
		extend(s)
		return r

	# Global scope and program

	fn program()
		skip_line()
		global_entries -> tok() == Token.EndOfFile
		if tok() != Token.EndOfFile
			expected(Token.EndOfFile)

	fn global_entries(term)
		loop
			if !global_entry()
					break
			if term()
				break
			sep()

	fn global_entry() -> bool constraints
		match tok()
			when Token.Ident
				match lexer.tok.symbol
					when Symbol.kw_data
						data()
						return true
					else
						attribute()
						return true
			else
				return false

		return true

	fn global_scope(baseline)
		parse_scope(baseline, global_entries)

	fn parse_scope(baseline, term)
		if eq(Token.Line)
			if lexer.indent_newline(baseline)
				var r = term(-> eq(Token.Deindent))

				if eq(Token.Deindent)
					lexer.next_token_after_deindent()
				else
					expected(Token.Deindent)

				return some r

		var state = lexer.tok

		skip_line()

		return if matches(Token.BraceOpen)
			skip_line()
			var r2 = term(-> eq(Token.BraceClose))
			ensure(Token.BraceClose)
			some r2
		else
			lexer.tok = state
			nil

	fn properties()
		loop
			if matches_ident(Symbol.kw_shared)
			else if matches_ident(Symbol.kw_import)
			else if matches_ident(Symbol.kw_export)
			else
				if matches(Token.SquareOpen)
					skip_line()

					if !eq(Token.SquareClose)
						loop
							get_ident()

							if matches(Token.Comma)
								skip_line()
							else
								break

					ensure(Token.SquareClose)

				return

	fn attribute()
		src \s ->
			var props = properties()
			var baseline = lexer.tok.indent

			if(matches_ident(Symbol.kw_fn))
				fn(s, baseline, get_ident(), props, nil)
			else
				var name = undef()
				var type = undef()
				var val
				(name, type) = name_opt_type()

				if matches(Token.Assign)
					skip_line()
					val = some expression()

	fn fn_params()
		skip_line()

		if is_type_expression()
			loop
				name_opt_type()

				if matches(Token.Comma)
					skip_line()
				else
					break

	fn fn(s, baseline, name, props, Option[Symbol*] action_type)
		var tp = kind_params()

		if matches(Token.Mul)
			src \s ->
				name_opt_type()
		else
			if action_type.has() and !eq(Token.ParentOpen)
			else
				ensure(Token.ParentOpen)
				fn_params()
				ensure(Token.ParentClose)

		if action_type.has()
			if matches(Token.Colon)
				skip_line()

				loop
					get_ident()
					ensure(Token.ArrowLeft)
					expression()

					if matches(Token.Comma)
						skip_line()
					else
						break
		else
			if matches(Token.ArrowRight)
				skip_line()

				type_expression()

				matches_ident(Symbol.kw_constraints)

		var block = group(baseline)
		return ast[DefNode.Function](s, block)


	fn data()
		var s = lexer.src()
		var baseline = lexer.tok.indent
		step()
		var sizeable = !matches_ident(Symbol.kw_ref)
		var name = get_ident()
		kind_params()
		extend s
		var scope = global_scope(baseline)

	fn kind_params() -> () constraints
		src \s ->
			if matches(Token.SquareOpen)
				skip_line()
				var params = type_params()
				ensure(Token.SquareClose)

	fn type_args(Array[TypeNode*, Region]* args)
		loop
			type_operator()
			if matches(Token.Comma)
				skip_line()
			else
				break

	fn type_params()
		src \s ->
			var type = type_expression()
			var value = false
			var name = Symbol.none
			if eq(Token.Ident)
				name = get_ident()
			else if matches(Token.DoubleColon)
				skip_line()
				name = get_ident()
				value = true
			else
				type = TypeNode.none

			kind_params()
			if matches(Token.Assign)
				skip_line()
				if value
					expression
				else
					type_expression

	fn is_type_expression()
		return match tok()
			when Token.Ident, Token.Number, Token.String, Token.ParentOpen
				true
			else
				false

	fn type_operator()
		var r = type_expression()

		loop
			if !eq(Token.ArrowRight)
				break

			step()
			skip_line()

			type_expression()

		return r

	fn type_expression() -> TypeNode* constraints
		return type_ptr()

	fn type_ptr()
		var r = type_chain()

		loop
			if !eq(Token.Mul)
				break

			step()

		return r

	fn type_chain()
		var r = type_factor()

		loop
			match tok()
				when Token.SquareOpen
					step()
					skip_line()
					var args = array()
					type_args(&args)
					ensure(Token.SquareClose)
				when Token.Dot
					step()
					skip_line()
					var name = get_ident()
				else
					break

		return r

	fn type_tuple(s)
		step()
		skip_line()

		var nodes = array()

		if !eq(Token.ParentClose)
			type_args(&nodes)

		ensure(Token.ParentClose)

		return ast[TypeNode.Tuple](s, nodes)

	fn type_factor()
		return src \s ->
			return match tok()
				when Token.ParentOpen
					type_tuple(s)
				when Token.Number
					var r = ast[TypeNode.Number](s)
					step()
					r
				when Token.Ident
					if matches_ident(Symbol.kw_typeof)
						chain()
						ast[TypeNode.Typeof](s)
					else
						var r2 = ast[TypeNode.Ref](s, lexer.tok.symbol)
						step()
						r2
				else
					expected_str("type")
					TypeNode.none

	fn name_opt_type()
		var type = type_expression()
		var name = undef()

		if eq(Token.Ident)
			name = get_ident()
		else
			match *type as node
				when TypeNode.Ref
					name = node.name
					type = TypeNode.none
				else
					expected_str_s(type.src, "variable name")

		return (name, type)

	fn group(baseline) -> Block*
		var block = ast[Block]()

		parse_scope(baseline, \term ->
			loop
				if !is_expression()
					break
				expression()
				if term()
					break
				sep())

		return block

	fn is_expression()
		if is_factor()
			return true

		return match tok()
			when Token.Ident, Token.Mul, Token.And, Token.Dot, Token.Sub, Token.Add, Token.Not, Token.Join
				true
			else
				false

	fn expression() -> ValueNode* constraints
		if eq(Token.Ident)
			match lexer.tok.symbol
				when Symbol.kw_var
					return _var()
				when Symbol.kw_return
					return _return()
				when Symbol.kw_if
					return _if()
				when Symbol.kw_match
					return _match()
				when Symbol.kw_loop
					return _loop()
				else

		return assign_operator()

	fn expression_list(Array[ValueNode*, Region]* args)
		loop
			args.push(expression())

			if matches(Token.Comma)
				skip_line()
			else
				break

	fn _var()
		return src \s ->
			step()
			skip_line()

			var name = undef()
			var type = undef()
			(name, type) = name_opt_type()
			var value = ValueNode.none

			if matches(Token.Assign)
				skip_line()
				value = expression()

			return ast[ValueNode.LocalVar](s, name, type, value)

	fn _return()
		return src \s ->
			step()
			skip_line()

			return ast[ValueNode.Return](s, if is_expression()
				expression()
			else
				ValueNode.none)

	fn _if() -> ValueNode*
		var baseline = lexer.tok.indent

		var s = lexer.src()
		step()
		skip_line()
		var exp = expression()
		extend(s)

		var block = group(baseline)

		var state = lexer.tok

		skip_line()

		var else_node = if eq_i(Symbol.kw_else)
			var else_baseline = lexer.tok.indent
			step()

			if eq_i(Symbol.kw_if)
				_if()
			else
				src \s -> ast[ValueNode.Group](s, group(else_baseline))
		else
			lexer.tok = state

			ValueNode.none

		return ast[ValueNode.If](s, exp, block, else_node)

	fn _match()
		return src \s ->
			var baseline = lexer.tok.indent
			step()
			skip_line()

			var exp = expression()
			var binding = nil

			if matches_ident(Symbol.kw_as)
				binding = some(src \s -> (s, get_ident()))

			parse_scope(baseline, \term ->
				loop
					if !eq_i(Symbol.kw_when)
						break

					src \s ->
						var when_baseline = lexer.tok.indent
						step()
						skip_line()
						var cases = array()
						expression_list(&cases)
						group(when_baseline)

					if term()
						break

					sep()

				if eq_i(Symbol.kw_else)
					var else_baseline = lexer.tok.indent
					step()
					group(else_baseline)
					if !term()
						sep()
				)

			match binding as b
				when Option.Some
				else

			return ValueNode.none

	fn _loop()
		var baseline = lexer.tok.indent
		var s = lexer.src()
		step()
		var block = group(baseline)
		return ast[ValueNode.Loop](s, block)

	fn assign_operator()
		var result = pred_op(type_assert(), 0)

		return match tok()
			when Token.Assign, Token.AssignLShift, Token.AssignRShift, Token.AssignAdd, Token.AssignSub, Token.AssignMul, Token.AssignDiv, Token.AssignMod, Token.AssignOr, Token.AssignAnd, Token.AssignXor, Token.AssignJoin
				var s = lexer.src()
				var op = tok()
				step()
				skip_line()
				ast[ValueNode.Assign](s, result, op, expression())
			else
				result

	Map[Token, uint] PredOps

	fn setup_ops()
		var i = 1

		PredOps.set(Token.LogicalOr, i)
		i += 1

		PredOps.set(Token.LogicalAnd, i)
		i += 1

		PredOps.set(Token.Eq, i)
		PredOps.set(Token.NotEq, i)
		i += 1

		PredOps.set(Token.Or, i)
		i += 1

		PredOps.set(Token.Xor, i)
		i += 1

		PredOps.set(Token.And, i)
		i += 1

		PredOps.set(Token.LessOrEq, i)
		PredOps.set(Token.GreaterOrEq, i)
		PredOps.set(Token.Less, i)
		PredOps.set(Token.Greater, i)
		i += 1

		PredOps.set(Token.RShift, i)
		PredOps.set(Token.LShift, i)
		i += 1

		PredOps.set(Token.Add, i)
		PredOps.set(Token.Sub, i)
		PredOps.set(Token.Join, i)
		i += 1

		PredOps.set(Token.Mul, i)
		PredOps.set(Token.Div, i)
		PredOps.set(Token.Mod, i)


	fn current_pred_op()
		return if eq_i(Symbol.kw_and)
			Token.LogicalAnd
		else if eq_i(Symbol.kw_or)
			Token.LogicalOr
		else
			tok()

	fn is_pred_op()
		return PredOps.has(current_pred_op())

	fn pred_op(left, min) -> ValueNode*
		loop
			if !is_pred_op()
				break

			var op = current_pred_op()
			var op_src = lexer.src()
			var pred = PredOps.assert_get(op)

			if pred < min
				break

			step()
			skip_line()

			var right = type_assert()

			loop
				if !is_pred_op()
					break

				var next_pred = PredOps.assert_get(current_pred_op())

				if next_pred <= pred
					break

				right = pred_op(right, next_pred)

			left = ast[ValueNode.BinOp](op_src, left, op, right)

		return left

	fn type_assert()
		var r = unary()


		return if eq(Token.DoubleColon)
			src \s ->
				step()
				skip_line()

				return ast[ValueNode.TypeAssert](s, r, type_expression())
		else
			r

	fn unary() -> ValueNode*
		return match tok()
			when Token.And, Token.Mul, Token.Sub, Token.Add, Token.Not, Token.Join
				src \s ->
					var op = tok()
					step()
					skip_line()
					return ast[ValueNode.UnaryOp](s, op, unary())
			else
				subscript()

	fn subscript() -> ValueNode*
		var r = apply()
		return if eq(Token.Subscript)
			src \s ->
				step()
				skip_line()
				return ast[ValueNode.Subscript](s, r, subscript())
		else
			r

	fn apply()
		var s = lexer.src()
		var result = chain()

		loop
			if !is_factor()
				break

			var new_s = lexer.src()
			var arg = chain()
			var args = array()
			args.push(arg)
			extend(s)
			result = ast[ValueNode.Call](s, result, args)
			s = new_s

		return result

	fn chain()
		var result = factor()

		loop
			match tok()
				when Token.ParentOpen
					src \s ->
						step()
						skip_line()

						var args = array()

						if !eq(Token.ParentClose)
							expression_list(&args)

						ensure(Token.ParentClose)

						result = ast[ValueNode.Call](s, result, args)
				when Token.SquareOpen
					src \s ->
						step()
						skip_line()

						var args = array()
						type_args(&args)

						ensure(Token.SquareClose)

						result = ast[ValueNode.TypeArgs](s, result, args)
				when Token.Dot
					src \s ->
						step()
						skip_line()

						result = ast[ValueNode.Field](s, result, get_ident())
				else
					break

		return result

	fn tuple(s)
		step()
		skip_line()

		var nodes = array()

		if !eq(Token.ParentClose)
			expression_list(&nodes)

		ensure(Token.ParentClose)

		if nodes.size == 1
			return nodes'0
		else
			return ast[ValueNode.Tuple](s, nodes)

	fn _array(s)
		step()
		skip_line()

		var nodes = array()

		if !eq(Token.SquareClose)
			expression_list(&nodes)

		ensure(Token.SquareClose)

		return ast[ValueNode.ArrayLit](s, nodes)

	fn lambda()
		var baseline = lexer.tok.indent

		src \s ->
			if matches(Token.Backslash)
				fn_params()
			ensure(Token.ArrowRight)

		if eq(Token.Line) or eq(Token.BraceOpen)
			group(baseline)
		else
			expression()

		return ValueNode.none


	fn is_factor()
		return match tok()
			when Token.SquareOpen, Token.Backslash, Token.ParentOpen, Token.ArrowRight, Token.Number, Token.String
				true
			when Token.Ident
				match lexer.tok.symbol
					when Symbol.kw_or, Symbol.kw_and
						false
					when Symbol.kw_as
						true  
					else
						true
			else
				false

	fn factor()
		return src \s ->
			return match tok()
				when Token.SquareOpen
					_array(s)
				when Token.ParentOpen
					tuple(s)
				when Token.Backslash, Token.ArrowRight
					lambda()
				when Token.Number, Token.String
					step()
					ValueNode.none
				when Token.Ident
					match lexer.tok.symbol
						when Symbol.kw_nil
							step()
							ValueNode.none
						else
							ast[ValueNode.Ref](s, get_ident())
				else
					expected_str("expression")
					ValueNode.none